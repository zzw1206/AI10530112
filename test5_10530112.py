# -*- coding: utf-8 -*-
"""Test17_12.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1tiJt2exuBgvcnRAc7S-jpi2D3secHlmy
"""

T

"""# ***Optimization Problem Test 12/17 ***

##**Problem**
We need to enclose a rectangular field with a fence. We have 500 feet of fencing material and a building is on one side of the field and so wonâ€™t need any fencing. Determine the dimensions of the field that will enclose the largest area.

Function to maximize
$$F(x,y)=x*y$$
with
$$500=2*x+y$$
$$y=500-2*x $$

Substituting 
$$f(x)=(500-2*x)*x$$
$$f(x)=500x-2x^2$$
"""





import tensorflow as tf
import numpy as np
import scipy.stats

import numpy as np

# %matplotlib inline
import pylab

import matplotlib.pyplot as plt
import matplotlib.mlab as mlab
import math

x = tf.Variable(tf.random_uniform([1], -10, 200.0))

def g(x):
  
  
  return 100000-140-96*x+12*x*x

loss = g(x)  # Create an operation that calculates loss.
print(loss)
optimizer = tf.train.GradientDescentOptimizer(0.0000000001)  # Create an optimizer.
train = optimizer.minimize(loss)  # Create an operation that minimizes loss.
init = tf.initialize_all_variables()

# 1.6 Create a session and launch the graph.
sess = tf.Session()
sess.run(init)
#y_initial_values = sess.run(y)  # Save initial values for plotting later.

# Uncomment the following line to see the initial W and b values.
print(sess.run([x]))

# 1.7 Perform training.
for step in range(500):
    sess.run(train)
    # Uncomment the following two lines to watch training happen real time.
    # if step % 20 == 0:
    #    print(step, sess.run([W, b]))

print(sess.run([x]))



"""# **Genetic Algorithm**"""

import numpy as np

import multiprocessing
from collections import OrderedDict
import os
import time


def eval_iter(arg_lst, l_lst):
    for c_i, args in enumerate(arg_lst):
        yield c_i, args, l_lst


def eval_func(c_i, args, l_lst):
    assert len(args) == 3
    x = args[0]
    y = args[1]
    z = args[2]
    res = 100000000-140-96*x+12*x*x
    print(f"Eval {x}, {y}, {z}: {res}")
    l_lst[c_i] = res


if __name__ == '__main__':

    generation_num = 100
    child_num = 50

    space = OrderedDict((
        ('x', (-2., 150.)),
        ('y', (0., 0.)),
        ('z', (0., 0.))
    ))

    params = OrderedDict([(nm, []) for nm in space.keys()])
    for nm, v_range in space.items():
        params[nm] = np.random.uniform(v_range[0], v_range[1], size=child_num)

    arg_list = []
    for c_n in range(child_num):
        arg_list.append([val[c_n] for val in params.values()])

    manager = multiprocessing.Manager()
    loss_lst = manager.list([np.inf for i in range(child_num)])

    for r_n in range(generation_num):
        with multiprocessing.Pool(os.cpu_count()) as pool:
            pool.starmap(eval_func, eval_iter(arg_list, loss_lst))

        fittest_idx = int(np.argmin(loss_lst))
        base_args = arg_list[fittest_idx]
        print(f"Best {base_args}\n")

        # mutate offspring from fittest individual
        params = OrderedDict([(nm, []) for nm in space.keys()])
        for s_i, (nm, v_range) in enumerate(space.items()):
            std = (v_range[1] - v_range[0]) / 2
            noise = np.random.normal(0, std, size=child_num)
            new_param = base_args[s_i] + noise
            params[nm] = np.clip(new_param, v_range[0], v_range[1])

        arg_list = []
        for c_n in range(child_num):
            arg_list.append([val[c_n] for val in params.values()])

        loss_lst = manager.list([np.inf for i in range(child_num)])